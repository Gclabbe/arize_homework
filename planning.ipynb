{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac207c92-37e5-4dbf-b15a-2e6480866f6f",
   "metadata": {},
   "source": [
    "# Integration of Langchain & Phoenix\n",
    "\n",
    "[Working notebook](https://colab.research.google.com/drive/15YcUJ82BHqjxO3JaqiMbXf6ymG6UxpyE?usp=sharing)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a233c7a8-84db-49c6-81ce-3f6b34ee47f4",
   "metadata": {},
   "source": [
    "## Results\n",
    "After working through a couple of different ways to use Langchain for a RAG for Shakespeare, I settled on OpenAI as the core LLM for this exercise.  Smaller locally trained models (Bloomz-7b, Falcon7b) were not doing well.\n",
    "\n",
    "My first plan had been to get Phoenix Eval working well enough to really show the potential difference in performance between different LLM options since the beauty of these chain structures is the ability to switch pieces out.\n",
    "\n",
    "### Here's what the end result consists of\n",
    "* Ask GPT-4 to write 20 questions about Shakespeare books\n",
    "* Ask GPT-4 to answer it's own questions\n",
    "* Download and parse the collection\n",
    "* Build embeddings into ChromaDB\n",
    "* For each question\n",
    "    * Generate answer through the Chat chain\n",
    "    * Pull the set of retrievals used to create the answer\n",
    "* Use Phoenix to assess the quality of the retrievals\n",
    "* Create output report from Eval\n",
    "\n",
    "There were definitely some challenges getting this all hooked together.  I very intentionally avoided looking at your example notebooks until I was getting stuck at some points.  To be fair, a lot of what I was doing would have been easier with the tutorials you have online.  At least in the main point of getting to the operation of Phoenix and less about making the Chat system function correctly.\n",
    "\n",
    "### Ideas on how to make the process easier and more effective\n",
    "\n",
    "* How can we integrate Eval into the langchain directly?\n",
    "* Conda environment.yml on startup / install ...\n",
    "    * not just our package, everything needed to get operating in a clean environment\n",
    "* <b>Is there a better process to help the users generate the truth?</b> -- this is the biggest challenge I see right now.\n",
    "* <b>Can we automate the prototyping using LLMs?</b>  -- and this is the biggest potential opportunity\n",
    "    * i.e. point Phoenix at the data and we do the rest \n",
    "    * load, chain, construct challenges & truths, then test against several models\n",
    " \n",
    "### How to get the truth for Eval?\n",
    "The obvious answer is for a person to invest the time in labeling their own data.  However, this is a potential impediment for adoption.  That concept of hand labeling 10s or 100s of items is just enough to potenially put a user into the \"maybe later\" mindset.\n",
    "\n",
    "The approach I decided to play around with is the potential to auto-gen A truth.\n",
    "\n",
    "This is similar to a project (tiny startup) that I kicked off in October 2021.  The idea was for us to massively accelerate the process for generating labeled image data for training with MaskRNN to strengthen SOTA image recognition models.  All the user would have to do is tell us what the next set of images was about, then draw a bounding box.  Our software would then create the outlined image of the most important thing in the box.  You can see the project here if you are interested\n",
    "[Scuba Steve Rapid Image Annotation](https://github.com/Gclabbe/scuba_steve)\n",
    "\n",
    "How does that apply?  Maybe we can pre-truth the customer's data through LLM assessment, then generate an evaluation result.  This will certainly not be correct, however, the results would be an interactive session of \"do you agree with this truth\" to help them label their data.  If it's a large dataset, sample 10% then re-generate until the errors are dropping and they have confidence.\n",
    "\n",
    "For this test of the idea I took the Answer that originally came from GPT-4 and the retrievals from the Chat completion and asked GPT-4 if the retrievals were good.  This essentially returned an answer or a phrase that indicated the retrievals were not good.  This was accepted as the truth.\n",
    "\n",
    "Next, feed it into the process for RAG assessment in Phoenix.\n",
    "\n",
    "Since it's basically GPT-4 against GPT-4, the premise was that there would be high accuracy ... there wasn't.\n",
    "\n",
    "I didn't feel like reading a lot of Shakespeare last night, so I didn't go further.  The stage is set, though.  At this point it would be trivial to loop through the items and ask if we preferred the Generated Truth or the Phoenix assessment.  The former meaning a true problem was found.  The latter meaning we update the truth to match the Phoenix result.\n",
    "\n",
    "### And this leads directly towards the idea of automating the process\n",
    "Can we just ask the user to point us at a portion of their data, massage the data into a data chat system, prompt them with questions if needed to understand the goals of the model, assemble the chain, generate a truth and fine-tune the truth?\n",
    "\n",
    "And take that further (mentioned below) into looping across key parameters once we have some stronger set of truth labels to check on chunk size, overlap, number of chunks, LLM options, etc.\n",
    "\n",
    "Basically, make it very hard to walk away from Phoenix as we try to fully automate their investigation and help them get to production.  At which point, Arize swoops in as the production observability system to monitor their new app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46eb172-0b4c-46f8-94bd-5ebcadcbff91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b825e11-808e-48a8-bbbb-aeea162f2d5b",
   "metadata": {},
   "source": [
    "# Lots of other ideas started popping out\n",
    "Let me know if you'd like to talk through any of these live.  There's more bouncing around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11befdc-1a5c-4158-9a2a-3bc60bcd3e8b",
   "metadata": {},
   "source": [
    "# Advancement concepts\n",
    "* Look into matching the output style of RoBerta for entailment\n",
    "    * supporting, irrelevant, not-supporting \n",
    "* Explore auto-tuning ... become the Optuna of LLMs projects\n",
    "    * Maybe integration of Optuna directly???\n",
    "    * Model parameters\n",
    "    * Key components like chunk size & overlap\n",
    "    * Randomization of chunks\n",
    "    * Prompt style suggestions\n",
    "* How to on <insert Cloud structure here>\n",
    "    * Google Cloud\n",
    "    * AWS Bedrock\n",
    "    * Azure <whatever>\n",
    "    * WhiteSpace\n",
    "    * Kaggle contest integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f82f3-1025-4e03-8d13-ff2b2c30695d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e8c5d-7db5-4f1f-a7c4-0af6c0b56ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66fc9834-35ff-437e-865c-8e06a73dc816",
   "metadata": {},
   "source": [
    "# Ideas on improvements to the metrics interface on Arize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f7aea-2d10-4115-9a26-605c07635a18",
   "metadata": {},
   "source": [
    "## Simplify the interface\n",
    "* I get disoriented everytime I'm in there (maybe weekly)\n",
    "* Too many options for detail views + dropdowns to build out a view\n",
    "* Limits to amount of data being uploaded for prompts and responses needs to be fixed\n",
    "* Automated clustering and grouping\n",
    "    * When there are obvious outliers, don't make me play a video game to isolate them\n",
    "    * Easy clustering\n",
    "    * Then, just send sample from the different sets and let <foundation model> tell me what is different\n",
    "* Multi-lingual options -- easier than every today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a8b80-1ae3-463c-ac25-190ebcb026e4",
   "metadata": {},
   "source": [
    "## Autogen summaries using foundation model NLG\n",
    "\"customer X has shown a recent change in metric Y\"  \n",
    "\"the new model release has change accuracy by X%, however, precision seems to be down\"\n",
    "\n",
    "This can probably actually be done through NLG templating\n",
    "* Use the foundation model to generate templates that match the customer's language / features / model names, etc\n",
    "* Fill in the blanks of the template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c049b7-9811-4419-8ae3-a59d39c9da71",
   "metadata": {},
   "source": [
    "## Automated monitoring\n",
    "* Beyond dashboarding with so many things to click\n",
    "* Simplify the entire view to more modern HTML5 type interface\n",
    "* Block style model / customer / metrics with summary info and click-to-view details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bf744-625b-42d0-a86f-22c7223b6024",
   "metadata": {},
   "source": [
    "## Integrated Phoenix Evaluation on RLHF\n",
    "* @ Company-V, we were sending RLHF opinions and comments\n",
    "    * Give value to RLHF component\n",
    "    * Downgrade negative sentiment if comment is more positive (reduce bias)\n",
    "    * On large population sets, search for bias (i.e. a particular customer \"always rejects the probability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbfd8aa-021b-4130-9500-297531638067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d182e-ec77-458d-a15a-f79821087fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9687d4-a8f2-4d6c-840b-63396988426b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
