{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEKghJQ2pmYH",
    "tags": []
   },
   "source": [
    "### RAG on LangChain meets Phoenix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "0TTosnCHnGHG",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import openai\n",
    "\n",
    "import dotenv\n",
    "\n",
    "from rich import print\n",
    "\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DUNhabQUB8f"
   },
   "source": [
    "### PromptTemplates\n",
    "\n",
    "Next stop, we'll discuss a few templates. This allows us to easily interact with our model by not having to redo work we've already completed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "74vpojywT0-4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "# we can signify variables we want access to by wrapping them in {}\n",
    "system_prompt_template = \"You are an expert in {SUBJECT}, and you're currently feeling {MOOD}\"\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(system_prompt_template)\n",
    "\n",
    "user_prompt_template = \"{CONTENT}\"\n",
    "user_prompt_template = HumanMessagePromptTemplate.from_template(user_prompt_template)\n",
    "\n",
    "# put them together into a ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt_template, user_prompt_template])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-nbEW-kV_na"
   },
   "source": [
    "Now that we have our `chat_prompt` set-up with the templates - let's see how we can easily format them with our content!\n",
    "\n",
    "NOTE: `disp_markdown` is just a helper function to display the formatted markdown response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "P4vd-W2FV7Xq",
    "outputId": "2c4bda02-7ba3-4d72-f6d2-dddeed8c3385",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note the method `to_messages()`, that's what converts our formatted prompt into \n",
    "# formatted_chat_prompt = chat_prompt.format_prompt(SUBJECT=\"cheeses\", MOOD=\"quite tired\", CONTENT=\"Hi, what are the finest cheeses?\").to_messages()\n",
    "\n",
    "# disp_markdown(chat_model(formatted_chat_prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHehNFjAXbU_"
   },
   "source": [
    "### Setting up the LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "lTzw4ZMoWX0X",
    "outputId": "ed63bd6c-a682-4a95-fbc8-ad5fc678574f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=chat_model, prompt=chat_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md5XYaAj_t51"
   },
   "source": [
    "### Load up the target book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "HX00sL92LATv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/guide1.txt\") as f:\n",
    "    hitchhikersguide = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PdfLcOlKcjH"
   },
   "source": [
    "Next we'll want to split our text into appropirately sized chunks. \n",
    "\n",
    "We're going to be using the [CharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/character_text_splitter.html) from LangChain today.\n",
    "\n",
    "The size of these chunks will depend heavily on a number of factors relating to which LLM you're using, what the max context size is, and more. \n",
    "\n",
    "You can also choose to have the chunks overlap to avoid potentially missing any important information between chunks. As we're dealing with a novel - there's not a critical need to include overlap.\n",
    "\n",
    "We can also pass in the separator - this is what we'll try and separate the documents on. Be careful to understand your documents so you can be sure you use a valid separator!\n",
    "\n",
    "For now, we'll go with 1000 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "BSYNeLXPKZtn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator = \"\\n\")\n",
    "texts = text_splitter.split_text(hitchhikersguide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9w-svpbLq62",
    "outputId": "623c31d3-4676-4dcf-bb41-24df47a7234e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQCXLq-ML_aN"
   },
   "source": [
    "Now that we've split our document into more manageable sized chunks. We'll need to embed those documents!\n",
    "\n",
    "For more information on embedding - please check out [this](https://platform.openai.com/docs/guides/embeddings) resource from OpenAI.\n",
    "\n",
    "In order to do this, we'll first need to select a method to embed - for this example we'll be using OpenAI's embedding - but you're free to use whatever you'd like. \n",
    "\n",
    "You just need to ensure you're using consistent embeddings as they don't play well with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "VigAmqxaMd5a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEN_IgzqOBNs"
   },
   "source": [
    "Now that we've set up how we want to embed our document - we'll need to embed it. \n",
    "\n",
    "For this week we'll be glossing over the technical details of this process - as we'll get more into next week.\n",
    "\n",
    "Just know that we're converting our text into an easily queryable format!\n",
    "\n",
    "We're going to leverage ChromaDB for this example, so we'll want to install that dependency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-ZuzHPCOjLc",
    "outputId": "17641e2c-4838-4785-ddb6-4b14b5b0b3b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install chromadb tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ql7jqj7TONDE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfn0R64lPb7n"
   },
   "source": [
    "Now that we have our documents embedded we're free to query them with natural language! Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ubZwxCHvQzsT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What makes towels important?\"\n",
    "docs = docsearch.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4M08F78Q3i3",
    "outputId": "5364b9ec-b8ce-423a-cbd7-3baabf705bc4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"value - you can wrap it around you for warmth as you bound across\\nthe cold moons of Jaglan Beta; you can lie on it on the brilliant\\nmarble-sanded beaches of Santraginus V, inhaling  the  heady  sea\\nvapours;  you can sleep under it beneath the stars which shine so\\nredly on the desert world of Kakrafoon; use it  to  sail  a  mini\\nraft  down  the slow heavy river Moth; wet it for use in hand-to-\\nhand-combat; wrap it round your head to ward off noxious fumes or\\nto  avoid  the  gaze of the Ravenous Bugblatter Beast of Traal (a\\nmindboggingly stupid animal, it assumes that if you can't see it,\\nit  can't  see  you - daft as a bush, but very ravenous); you can\\nwave your towel in emergencies  as  a  distress  signal,  and  of\\ncourse  dry  yourself  off  with it if it still seems to be clean\\nenough.\\n \\nMore importantly, a towel has immense  psychological  value.  For\\nsome reason, if a strag (strag: non-hitch hiker) discovers that a\\nhitch hiker has his towel with him, he will automatically  assume\", metadata={'source': '36'})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8W9ZmNaRRBX"
   },
   "source": [
    "Finally, we're able to combine what we've done so far into a chain!\n",
    "\n",
    "We're going to leverage the `load_qa_chain` to quickly integrate our queryable documents with an LLM.\n",
    "\n",
    "There are 4 major methods of building this chain, they can be found [here](https://docs.langchain.com/docs/components/chains/index_related_chains)!\n",
    "\n",
    "For this example we'll be using the `stuff` chain type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "S7vAWKiFSVj_",
    "outputId": "2f44bc25-ea96-4c0f-9cc9-8f69c73d913e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Towels are important because they have immense psychological value and practical uses, such as providing warmth, protection from noxious fumes, and a distress signal.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "query = \"What makes towels important?\"\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMxm7pdwUs5K"
   },
   "source": [
    "Now that we have this set-up, we'll want to package it into an app and pass it to a Hugging Face Space!\n",
    "\n",
    "You can find instruction on how to do that in the GitHub Repository!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
